{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 124, 124, 8)       208       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 62, 62, 8)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 58, 58, 16)        3216      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 29, 29, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 25, 25, 100)       40100     \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 12, 12, 100)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 200)         500200    \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 4, 4, 200)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 17424)             55774224  \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1024)              17843200  \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 500)               512500    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 44)                22044     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74,695,692\n",
      "Trainable params: 74,695,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import metrics\n",
    "from model import neural_voice_judgment_model\n",
    "\n",
    "model = neural_voice_judgment_model()\n",
    "model.build((None,128,128,1))\n",
    "model.summary()\n",
    "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\",metrics=metrics.TopKCategoricalAccuracy(k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "45/45 [==============================] - 19s 410ms/step - loss: 0.1063 - top_k_categorical_accuracy: 0.9556\n",
      "Epoch 2/500\n",
      "45/45 [==============================] - 18s 409ms/step - loss: 0.1212 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3/500\n",
      "45/45 [==============================] - 18s 410ms/step - loss: 0.1212 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4/500\n",
      "45/45 [==============================] - 19s 412ms/step - loss: 0.1061 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5/500\n",
      "45/45 [==============================] - 19s 412ms/step - loss: 0.1136 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6/500\n",
      "45/45 [==============================] - 18s 410ms/step - loss: 0.1086 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7/500\n",
      "45/45 [==============================] - 19s 411ms/step - loss: 0.1061 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      "45/45 [==============================] - 18s 410ms/step - loss: 0.0859 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9/500\n",
      "45/45 [==============================] - 19s 418ms/step - loss: 0.1263 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      "45/45 [==============================] - 19s 411ms/step - loss: 0.1313 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      "45/45 [==============================] - 18s 408ms/step - loss: 0.1136 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12/500\n",
      "45/45 [==============================] - 18s 409ms/step - loss: 0.1237 - top_k_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sample_yomikomi import sample_all_data\n",
    "from keras.callbacks import EarlyStopping\n",
    "import idx2numpy\n",
    "import tensorflow as tf\n",
    "from os.path import join\n",
    "from twenty_times_more_data import twenty_times_more_data\n",
    "\n",
    "input_output_label_data = idx2numpy.convert_from_file(join(\"..\",\"dataset\",\"train_label.idx\"))\n",
    "input_output_image_data = idx2numpy.convert_from_file(join(\"..\",\"dataset\",\"train_image.idx\"))\n",
    "label_dataset = tf.data.Dataset.from_tensor_slices(input_output_label_data)\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(input_output_image_data)\n",
    "lump_dataset = tf.data.Dataset.zip((image_dataset,label_dataset))\n",
    "callback = EarlyStopping(patience=10,monitor=\"top_k_categorical_accuracy\")\n",
    "flat_map_dataset = lump_dataset.flat_map(twenty_times_more_data)\n",
    "fit_data = model.fit(flat_map_dataset,epochs=500,steps_per_epoch=45,callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "test_label_data = idx2numpy.convert_from_file(join(\"..\",\"dataset\",\"test_label.idx\"))\n",
    "test_image_data = idx2numpy.convert_from_file(join(\"..\",\"dataset\",\"test_image.idx\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 3s 91ms/step - loss: 0.1087 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1087045669555664, 1.0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=test_image_data,y=test_label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trained_model\\second_tf\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(join(\"trained_model\",\"second.tf\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "971a761953a7663b5da1095a7f59a039d6dc8b44ef25d8b62f53ff15b5106e5a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
